# æ•°æ®è´¨é‡æ¸…æ´—ç³»ç»Ÿ - æŠ¥å‘Šç³»ç»Ÿè®¾è®¡

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æŠ¥å‘Šç±»å‹](#æŠ¥å‘Šç±»å‹)
- [æŠ¥å‘Šç”Ÿæˆå™¨](#æŠ¥å‘Šç”Ÿæˆå™¨)
- [æŠ¥å‘Šæ ¼å¼](#æŠ¥å‘Šæ ¼å¼)
- [è´¨é‡æŒ‡æ ‡](#è´¨é‡æŒ‡æ ‡)
- [è¶‹åŠ¿åˆ†æ](#è¶‹åŠ¿åˆ†æ)
- [å¯è§†åŒ–](#å¯è§†åŒ–)

---

## æ¦‚è¿°

æŠ¥å‘Šç³»ç»Ÿæ˜¯æ•°æ®æ¸…æ´—ç³»ç»Ÿçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œæä¾›è¯¦ç»†çš„æ¸…æ´—ç»“æœã€é”™è¯¯åˆ†æå’Œæ”¹è¿›å»ºè®®ã€‚

### æŠ¥å‘Šç±»å‹

1. **æ‰¹æ¬¡æŠ¥å‘Š**ï¼šæ¯æ¬¡æ¸…æ´—ä»»åŠ¡çš„è¯¦ç»†æŠ¥å‘Š
2. **å‘¨æœŸæŠ¥å‘Š**ï¼šæŒ‰å¤©/å‘¨/æœˆæ±‡æ€»çš„è´¨é‡æŠ¥å‘Š
3. **è¶‹åŠ¿æŠ¥å‘Š**ï¼šæ•°æ®è´¨é‡è¶‹åŠ¿åˆ†æ
4. **å‘Šè­¦æŠ¥å‘Š**ï¼šä¸¥é‡é”™è¯¯æˆ–å¼‚å¸¸æƒ…å†µçš„å³æ—¶é€šçŸ¥

---

## æŠ¥å‘Šç”Ÿæˆå™¨

### QualityReporter ç±»

**æ–‡ä»¶**: `backend/app/cleaning/reporting/quality_reporter.py`

```python
from typing import List, Dict
from datetime import datetime, timedelta
from backend.app.cleaning.models import ErrorRecord, FixResult
from backend.app.db.repository import DataQualityLogRepository, DataCleaningBatchRepository

class QualityReporter:
    """è´¨é‡æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.log_repo = DataQualityLogRepository()
        self.batch_repo = DataCleaningBatchRepository()
    
    async def generate_report(
        self,
        batch_id: str,
        errors: List[ErrorRecord],
        fixes: List[FixResult]
    ) -> str:
        """ç”Ÿæˆæ‰¹æ¬¡æŠ¥å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰"""
        
        # 1. æ€»ä½“ç»Ÿè®¡
        summary = self._generate_summary(errors, fixes)
        
        # 2. é”™è¯¯åˆ†å¸ƒ
        error_dist = self._generate_error_distribution(errors)
        
        # 3. ä¿®å¤è¯¦æƒ…
        fix_details = self._generate_fix_details(fixes)
        
        # 4. æˆåŠŸæ¡ˆä¾‹
        success_cases = self._format_success_cases(fixes[:5])  # å‰ 5 ä¸ªæˆåŠŸæ¡ˆä¾‹
        
        # 5. å¤±è´¥æ¡ˆä¾‹
        failed_cases = self._format_failed_cases(
            [f for f in fixes if not f.success][:5]
        )
        
        # 6. æ”¹è¿›å»ºè®®
        recommendations = self._generate_recommendations(errors, fixes)
        
        # ç»„è£…å®Œæ•´æŠ¥å‘Š
        report = f"""
# æ•°æ®è´¨é‡æ¸…æ´—æŠ¥å‘Š

**æ‰¹æ¬¡ ID**: `{batch_id}`  
**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

{summary}

---

## ğŸ” é”™è¯¯åˆ†å¸ƒ

{error_dist}

---

## ğŸ”§ ä¿®å¤è¯¦æƒ…

{fix_details}

---

## âœ… æˆåŠŸæ¡ˆä¾‹ï¼ˆTop 5ï¼‰

{success_cases}

---

## âŒ å¤±è´¥æ¡ˆä¾‹ï¼ˆéœ€è¦äººå·¥ä»‹å…¥ï¼‰

{failed_cases}

---

## ğŸ’¡ æ”¹è¿›å»ºè®®

{recommendations}

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        return report
    
    def _generate_summary(
        self,
        errors: List[ErrorRecord],
        fixes: List[FixResult]
    ) -> str:
        """ç”Ÿæˆæ€»ä½“ç»Ÿè®¡"""
        
        total_errors = len(errors)
        fixes_attempted = len(fixes)
        fixes_successful = sum(1 for f in fixes if f.success)
        fixes_failed = fixes_attempted - fixes_successful
        success_rate = (fixes_successful / fixes_attempted * 100) if fixes_attempted > 0 else 0
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        critical = sum(1 for e in errors if e.severity == 'critical')
        high = sum(1 for e in errors if e.severity == 'high')
        medium = sum(1 for e in errors if e.severity == 'medium')
        low = sum(1 for e in errors if e.severity == 'low')
        
        return f"""
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ£€æµ‹åˆ°çš„é”™è¯¯** | {total_errors} |
| **å°è¯•ä¿®å¤** | {fixes_attempted} |
| **ä¿®å¤æˆåŠŸ** | {fixes_successful} ({success_rate:.1f}%) |
| **ä¿®å¤å¤±è´¥** | {fixes_failed} |
| **Critical é”™è¯¯** | {critical} ğŸ”´ |
| **High é”™è¯¯** | {high} ğŸŸ  |
| **Medium é”™è¯¯** | {medium} ğŸŸ¡ |
| **Low é”™è¯¯** | {low} ğŸŸ¢ |
"""
    
    def _generate_error_distribution(
        self,
        errors: List[ErrorRecord]
    ) -> str:
        """ç”Ÿæˆé”™è¯¯åˆ†å¸ƒ"""
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        by_type = {}
        for error in errors:
            by_type[error.error_type] = by_type.get(error.error_type, 0) + 1
        
        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        by_category = {}
        for error in errors:
            by_category[error.error_category] = by_category.get(error.error_category, 0) + 1
        
        # ç”Ÿæˆè¡¨æ ¼
        type_table = "### æŒ‰é”™è¯¯ç±»å‹\n\n| ç±»å‹ | æ•°é‡ | å æ¯” |\n|------|------|------|\n"
        total = len(errors)
        for error_type, count in sorted(by_type.items(), key=lambda x: x[1], reverse=True):
            pct = count / total * 100
            type_table += f"| {error_type} | {count} | {pct:.1f}% |\n"
        
        category_table = "\n### æŒ‰é”™è¯¯ç±»åˆ«\n\n| ç±»åˆ« | æ•°é‡ | å æ¯” | ä¸¥é‡ç¨‹åº¦ |\n|------|------|------|----------|\n"
        category_severity = self._get_category_severity_map()
        for category, count in sorted(by_category.items(), key=lambda x: x[1], reverse=True):
            pct = count / total * 100
            severity = category_severity.get(category, 'UNKNOWN')
            type_table += f"| {category} | {count} | {pct:.1f}% | {severity} |\n"
        
        return type_table + category_table
    
    def _generate_fix_details(
        self,
        fixes: List[FixResult]
    ) -> str:
        """ç”Ÿæˆä¿®å¤è¯¦æƒ…"""
        
        if not fixes:
            return "æ— ä¿®å¤æ“ä½œ\n"
        
        # æŒ‰ä¿®å¤æ–¹æ³•ç»Ÿè®¡
        by_method = {}
        for fix in fixes:
            by_method[fix.fix_method] = by_method.get(fix.fix_method, 0) + 1
        
        # ä¿®å¤æˆåŠŸç‡ï¼ˆæŒ‰æ–¹æ³•ï¼‰
        success_by_method = {}
        for fix in fixes:
            if fix.fix_method not in success_by_method:
                success_by_method[fix.fix_method] = {'total': 0, 'success': 0}
            success_by_method[fix.fix_method]['total'] += 1
            if fix.success:
                success_by_method[fix.fix_method]['success'] += 1
        
        table = "| ä¿®å¤æ–¹æ³• | æ€»æ•° | æˆåŠŸ | å¤±è´¥ | æˆåŠŸç‡ |\n"
        table += "|----------|------|------|------|--------|\n"
        
        for method, stats in success_by_method.items():
            total = stats['total']
            success = stats['success']
            failed = total - success
            rate = success / total * 100 if total > 0 else 0
            table += f"| {method} | {total} | {success} | {failed} | {rate:.1f}% |\n"
        
        return table
    
    def _format_success_cases(
        self,
        fixes: List[FixResult]
    ) -> str:
        """æ ¼å¼åŒ–æˆåŠŸæ¡ˆä¾‹"""
        
        if not fixes:
            return "æ— æˆåŠŸæ¡ˆä¾‹\n"
        
        cases = []
        for i, fix in enumerate(fixes, 1):
            case = f"""
### æ¡ˆä¾‹ {i}

- **ä¿®å¤æ–¹æ³•**: {fix.fix_method}
- **åŸå§‹æ•°æ®**: {fix.original_country}, {fix.original_city}
- **ä¿®å¤å**: {fix.fixed_country}, {fix.fixed_city}
- **åæ ‡**: {fix.fixed_coordinates}
"""
            cases.append(case)
        
        return "\n".join(cases)
    
    def _format_failed_cases(
        self,
        fixes: List[FixResult]
    ) -> str:
        """æ ¼å¼åŒ–å¤±è´¥æ¡ˆä¾‹"""
        
        if not fixes:
            return "æ— å¤±è´¥æ¡ˆä¾‹ âœ…\n"
        
        cases = []
        for i, fix in enumerate(fixes, 1):
            case = f"""
### æ¡ˆä¾‹ {i}

- **ä¿®å¤æ–¹æ³•**: {fix.fix_method}
- **åŸå§‹æ•°æ®**: {fix.original_country}, {fix.original_city}
- **å¤±è´¥åŸå› **: {fix.failure_reason}
- **å»ºè®®**: éœ€è¦äººå·¥å®¡æ ¸
"""
            cases.append(case)
        
        return "\n".join(cases)
    
    def _generate_recommendations(
        self,
        errors: List[ErrorRecord],
        fixes: List[FixResult]
    ) -> str:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = []
        
        # åˆ†æé”™è¯¯æ¨¡å¼
        state_as_city_count = sum(1 for e in errors if e.error_category == 'state_as_city')
        if state_as_city_count > 10:
            recommendations.append(
                f"âš ï¸ **å·ç¼©å†™è¯¯è¯†åˆ«**: æ£€æµ‹åˆ° {state_as_city_count} ä¸ªå·ç¼©å†™è¢«è¯¯è¯†åˆ«ä¸ºåŸå¸‚ã€‚"
                "å»ºè®®ä¼˜åŒ– rule-based æå–å™¨çš„å·ç¼©å†™éªŒè¯é€»è¾‘ã€‚"
            )
        
        # åˆ†æä¿®å¤æˆåŠŸç‡
        llm_fixes = [f for f in fixes if 'llm' in f.fix_method]
        if llm_fixes:
            llm_success_rate = sum(1 for f in llm_fixes if f.success) / len(llm_fixes)
            if llm_success_rate < 0.8:
                recommendations.append(
                    f"âš ï¸ **LLM ä¿®å¤æˆåŠŸç‡åä½**: å½“å‰ä¸º {llm_success_rate:.1%}ã€‚"
                    "å»ºè®®ä¼˜åŒ– LLM prompt æˆ–è€ƒè™‘ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹ã€‚"
                )
        
        # åˆ†æ geocoding å¤±è´¥
        geo_null_count = sum(1 for e in errors if e.error_category == 'geocoding_null')
        if geo_null_count > 20:
            recommendations.append(
                f"âš ï¸ **Geocoding å¤±è´¥ç‡é«˜**: æ£€æµ‹åˆ° {geo_null_count} ä¸ªæ— æ³•è·å–åæ ‡çš„ä½ç½®ã€‚"
                "å»ºè®®æ£€æŸ¥ Nominatim è¿æ¥æˆ–è€ƒè™‘ä½¿ç”¨å¤‡ç”¨ geocoding æœåŠ¡ã€‚"
            )
        
        if not recommendations:
            recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œæš‚æ— æ”¹è¿›å»ºè®®ã€‚")
        
        return "\n\n".join(recommendations)
    
    def _get_category_severity_map(self) -> Dict[str, str]:
        """è·å–é”™è¯¯ç±»åˆ«çš„ä¸¥é‡ç¨‹åº¦æ˜ å°„"""
        return {
            'state_as_city': 'HIGH',
            'institution_as_city': 'MEDIUM',
            'department_as_city': 'MEDIUM',
            'low_confidence': 'LOW',
            'country_city_mismatch': 'HIGH',
            'missing_geo_data': 'HIGH',
            'geocoding_null': 'HIGH',
            'wrong_coordinates': 'CRITICAL',
            'coordinate_anomaly': 'HIGH',
        }
    
    async def generate_periodic_report(
        self,
        start_date: datetime,
        end_date: datetime,
        period: str = 'weekly'
    ) -> str:
        """ç”Ÿæˆå‘¨æœŸæŠ¥å‘Šï¼ˆå‘¨æŠ¥/æœˆæŠ¥ï¼‰"""
        
        # æŸ¥è¯¢æ—¶é—´æ®µå†…çš„æ‰€æœ‰æ‰¹æ¬¡
        batches = await self.batch_repo.get_batches_in_range(start_date, end_date)
        
        # æ±‡æ€»ç»Ÿè®¡
        total_errors = sum(b.errors_detected for b in batches)
        total_fixes = sum(b.fixes_attempted for b in batches)
        total_success = sum(b.fixes_successful for b in batches)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# æ•°æ®è´¨é‡{period}æŠ¥å‘Š

**æ—¶é—´èŒƒå›´**: {start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}

## æ€»ä½“æƒ…å†µ

- æ¸…æ´—æ‰¹æ¬¡: {len(batches)}
- æ£€æµ‹é”™è¯¯: {total_errors}
- ä¿®å¤æˆåŠŸ: {total_success}/{total_fixes} ({total_success/total_fixes*100:.1f}%)

## è¶‹åŠ¿å›¾

{self._generate_trend_chart(batches)}

## æ”¹è¿›æ•ˆæœ

{self._analyze_improvement(batches)}
"""
        
        return report
    
    def _generate_trend_chart(self, batches) -> str:
        """ç”Ÿæˆè¶‹åŠ¿å›¾ï¼ˆASCII å›¾è¡¨ï¼‰"""
        # TODO: å®ç°ç®€å•çš„ ASCII å›¾è¡¨æˆ–è¿”å›æ•°æ®ä¾›å‰ç«¯å¯è§†åŒ–
        return "è¶‹åŠ¿å›¾æ•°æ®ï¼ˆä¾›å¯è§†åŒ–ï¼‰\n"
    
    def _analyze_improvement(self, batches) -> str:
        """åˆ†ææ”¹è¿›æ•ˆæœ"""
        if len(batches) < 2:
            return "æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿\n"
        
        # å¯¹æ¯”é¦–å°¾æ‰¹æ¬¡
        first = batches[0]
        last = batches[-1]
        
        error_rate_first = first.errors_detected / first.total_authorships_checked
        error_rate_last = last.errors_detected / last.total_authorships_checked
        
        improvement = (error_rate_first - error_rate_last) / error_rate_first * 100
        
        if improvement > 0:
            return f"âœ… æ•°æ®è´¨é‡æå‡ {improvement:.1f}%\n"
        else:
            return f"âš ï¸ æ•°æ®è´¨é‡ä¸‹é™ {-improvement:.1f}%\n"
```

---

## æŠ¥å‘Šæ ¼å¼

### æ‰¹æ¬¡æŠ¥å‘Šç¤ºä¾‹

```markdown
# æ•°æ®è´¨é‡æ¸…æ´—æŠ¥å‘Š

**æ‰¹æ¬¡ ID**: `clean_20260127_020000`  
**ç”Ÿæˆæ—¶é—´**: 2026-01-27 02:15:32

---

## ğŸ“Š æ€»ä½“ç»Ÿè®¡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ£€æµ‹åˆ°çš„é”™è¯¯** | 156 |
| **å°è¯•ä¿®å¤** | 143 |
| **ä¿®å¤æˆåŠŸ** | 124 (86.7%) |
| **ä¿®å¤å¤±è´¥** | 19 |
| **Critical é”™è¯¯** | 5 ğŸ”´ |
| **High é”™è¯¯** | 68 ğŸŸ  |
| **Medium é”™è¯¯** | 71 ğŸŸ¡ |
| **Low é”™è¯¯** | 12 ğŸŸ¢ |

---

## ğŸ” é”™è¯¯åˆ†å¸ƒ

### æŒ‰é”™è¯¯ç±»å‹

| ç±»å‹ | æ•°é‡ | å æ¯” |
|------|------|------|
| extraction_error | 89 | 57.1% |
| geocoding_error | 54 | 34.6% |
| consistency_error | 13 | 8.3% |

### æŒ‰é”™è¯¯ç±»åˆ«

| ç±»åˆ« | æ•°é‡ | å æ¯” | ä¸¥é‡ç¨‹åº¦ |
|------|------|------|----------|
| state_as_city | 42 | 26.9% | HIGH |
| geocoding_null | 38 | 24.4% | HIGH |
| institution_as_city | 24 | 15.4% | MEDIUM |
| low_confidence | 20 | 12.8% | LOW |
| wrong_coordinates | 5 | 3.2% | CRITICAL |
| ... | ... | ... | ... |

---

## ğŸ”§ ä¿®å¤è¯¦æƒ…

| ä¿®å¤æ–¹æ³• | æ€»æ•° | æˆåŠŸ | å¤±è´¥ | æˆåŠŸç‡ |
|----------|------|------|------|--------|
| llm_openai | 89 | 76 | 13 | 85.4% |
| geocoding_retry | 54 | 48 | 6 | 88.9% |

---

## âœ… æˆåŠŸæ¡ˆä¾‹ï¼ˆTop 5ï¼‰

### æ¡ˆä¾‹ 1

- **ä¿®å¤æ–¹æ³•**: llm_openai
- **åŸå§‹æ•°æ®**: United States, MD
- **ä¿®å¤å**: United States, Baltimore
- **åæ ‡**: {'lat': 39.2904, 'lng': -76.6122}

### æ¡ˆä¾‹ 2

- **ä¿®å¤æ–¹æ³•**: llm_openai
- **åŸå§‹æ•°æ®**: China, Harvard Medical School
- **ä¿®å¤å**: United States, Boston
- **åæ ‡**: {'lat': 42.3601, 'lng': -71.0589}

...

---

## âŒ å¤±è´¥æ¡ˆä¾‹ï¼ˆéœ€è¦äººå·¥ä»‹å…¥ï¼‰

### æ¡ˆä¾‹ 1

- **ä¿®å¤æ–¹æ³•**: llm_openai
- **åŸå§‹æ•°æ®**: Japan, NULL
- **å¤±è´¥åŸå› **: Geocoding failed after LLM extraction
- **å»ºè®®**: éœ€è¦äººå·¥å®¡æ ¸åŸå§‹ affiliation å­—ç¬¦ä¸²

...

---

## ğŸ’¡ æ”¹è¿›å»ºè®®

âš ï¸ **å·ç¼©å†™è¯¯è¯†åˆ«**: æ£€æµ‹åˆ° 42 ä¸ªå·ç¼©å†™è¢«è¯¯è¯†åˆ«ä¸ºåŸå¸‚ã€‚å»ºè®®ä¼˜åŒ– rule-based æå–å™¨çš„å·ç¼©å†™éªŒè¯é€»è¾‘ã€‚

âš ï¸ **Geocoding å¤±è´¥ç‡é«˜**: æ£€æµ‹åˆ° 38 ä¸ªæ— æ³•è·å–åæ ‡çš„ä½ç½®ã€‚å»ºè®®æ£€æŸ¥ Nominatim è¿æ¥æˆ–è€ƒè™‘ä½¿ç”¨å¤‡ç”¨ geocoding æœåŠ¡ã€‚

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2026-01-27 02:15:32
```

---

## è´¨é‡æŒ‡æ ‡

### QualityMetrics ç±»

**æ–‡ä»¶**: `backend/app/cleaning/metrics/quality_metrics.py`

```python
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡è®¡ç®—å™¨"""
    
    async def calculate_metrics(
        self,
        batch_id: str
    ) -> Dict[str, float]:
        """è®¡ç®—è´¨é‡æŒ‡æ ‡"""
        
        batch = await self.batch_repo.get_by_batch_id(batch_id)
        
        if not batch:
            return {}
        
        # 1. é”™è¯¯ç‡
        error_rate = (
            batch.errors_detected / batch.total_authorships_checked
            if batch.total_authorships_checked > 0 else 0
        )
        
        # 2. ä¿®å¤æˆåŠŸç‡
        fix_success_rate = batch.fix_success_rate or 0
        
        # 3. æ•°æ®å®Œæ•´åº¦ï¼ˆæœ‰åœ°ç†æ•°æ®çš„æ¯”ä¾‹ï¼‰
        data_completeness = await self._calculate_data_completeness()
        
        # 4. Geocoding æˆåŠŸç‡
        geocoding_success_rate = await self._calculate_geocoding_success_rate()
        
        # 5. æå–å‡†ç¡®ç‡ï¼ˆä¼°ç®—ï¼‰
        extraction_accuracy = 1 - (
            batch.errors_by_type.get('extraction_error', 0) / 
            batch.total_authorships_checked
        )
        
        return {
            'error_rate': error_rate,
            'fix_success_rate': fix_success_rate,
            'data_completeness': data_completeness,
            'geocoding_success_rate': geocoding_success_rate,
            'extraction_accuracy': extraction_accuracy
        }
    
    async def _calculate_data_completeness(self) -> float:
        """è®¡ç®—æ•°æ®å®Œæ•´åº¦"""
        # æŸ¥è¯¢æœ‰åœ°ç†æ•°æ®çš„ authorships æ¯”ä¾‹
        total = await self.authorship_repo.count_all()
        with_geo = await self.authorship_repo.count_with_geo_data()
        return with_geo / total if total > 0 else 0
    
    async def _calculate_geocoding_success_rate(self) -> float:
        """è®¡ç®— geocoding æˆåŠŸç‡"""
        # æŸ¥è¯¢æœ‰åæ ‡çš„ location_key æ¯”ä¾‹
        total = await self.geocoding_cache_repo.count_all()
        with_coords = await self.geocoding_cache_repo.count_with_coords()
        return with_coords / total if total > 0 else 0
```

### æ ¸å¿ƒæŒ‡æ ‡å®šä¹‰

| æŒ‡æ ‡ | å®šä¹‰ | è®¡ç®—å…¬å¼ | ç›®æ ‡å€¼ |
|------|------|----------|--------|
| **é”™è¯¯ç‡** | æ£€æµ‹åˆ°é”™è¯¯çš„ authorships æ¯”ä¾‹ | errors_detected / total_checked | < 15% |
| **ä¿®å¤æˆåŠŸç‡** | ä¿®å¤æˆåŠŸçš„é”™è¯¯æ¯”ä¾‹ | fixes_successful / fixes_attempted | > 80% |
| **æ•°æ®å®Œæ•´åº¦** | æœ‰åœ°ç†æ•°æ®çš„ authorships æ¯”ä¾‹ | with_geo_data / total_authorships | > 85% |
| **Geocoding æˆåŠŸç‡** | æœ‰åæ ‡çš„ location æ¯”ä¾‹ | with_coords / total_locations | > 90% |
| **æå–å‡†ç¡®ç‡** | æå–æ­£ç¡®çš„æ¯”ä¾‹ï¼ˆä¼°ç®—ï¼‰ | 1 - extraction_errors / total | > 95% |

---

## è¶‹åŠ¿åˆ†æ

### ç”Ÿæˆè¶‹åŠ¿æ•°æ®

```python
async def get_trend_data(
    self,
    start_date: datetime,
    end_date: datetime,
    granularity: str = 'daily'
) -> List[Dict]:
    """è·å–è¶‹åŠ¿æ•°æ®"""
    
    batches = await self.batch_repo.get_batches_in_range(start_date, end_date)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„
    trend_data = []
    
    for batch in batches:
        metrics = await self.calculate_metrics(batch.batch_id)
        
        trend_data.append({
            'date': batch.started_at.strftime('%Y-%m-%d'),
            'error_rate': metrics['error_rate'],
            'fix_success_rate': metrics['fix_success_rate'],
            'data_completeness': metrics['data_completeness'],
            'geocoding_success_rate': metrics['geocoding_success_rate']
        })
    
    return trend_data
```

### è¶‹åŠ¿å›¾è¡¨æ•°æ®æ ¼å¼

```json
[
  {
    "date": "2026-01-20",
    "error_rate": 0.18,
    "fix_success_rate": 0.82,
    "data_completeness": 0.75,
    "geocoding_success_rate": 0.85
  },
  {
    "date": "2026-01-21",
    "error_rate": 0.16,
    "fix_success_rate": 0.85,
    "data_completeness": 0.78,
    "geocoding_success_rate": 0.87
  },
  ...
]
```

---

## å¯è§†åŒ–

### å‰ç«¯é›†æˆï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦åœ¨å‰ç«¯æ˜¾ç¤ºæ•°æ®è´¨é‡æŠ¥å‘Šï¼Œå¯ä»¥åˆ›å»º API ç«¯ç‚¹ï¼š

```python
# backend/app/api/routes/cleaning.py

@router.get("/cleaning/reports/{batch_id}")
async def get_cleaning_report(batch_id: str):
    """è·å–æ¸…æ´—æŠ¥å‘Š"""
    reporter = QualityReporter()
    report = await reporter.get_report_data(batch_id)
    return report

@router.get("/cleaning/trends")
async def get_quality_trends(
    start_date: str,
    end_date: str
):
    """è·å–è´¨é‡è¶‹åŠ¿æ•°æ®"""
    metrics = QualityMetrics()
    trends = await metrics.get_trend_data(
        datetime.fromisoformat(start_date),
        datetime.fromisoformat(end_date)
    )
    return trends
```

### Chart.js ç¤ºä¾‹

```typescript
// frontend/src/components/DataQualityDashboard.tsx

import { Line } from 'react-chartjs-2';

export function DataQualityTrends({ trends }) {
  const data = {
    labels: trends.map(t => t.date),
    datasets: [
      {
        label: 'Error Rate',
        data: trends.map(t => t.error_rate * 100),
        borderColor: 'rgb(255, 99, 132)',
        backgroundColor: 'rgba(255, 99, 132, 0.5)',
      },
      {
        label: 'Fix Success Rate',
        data: trends.map(t => t.fix_success_rate * 100),
        borderColor: 'rgb(75, 192, 192)',
        backgroundColor: 'rgba(75, 192, 192, 0.5)',
      },
    ],
  };

  return <Line data={data} />;
}
```

---

## æŠ¥å‘Šå­˜å‚¨

### å­˜å‚¨ç­–ç•¥

1. **æ•°æ®åº“å­˜å‚¨**: æŠ¥å‘Šå…ƒæ•°æ®å’Œæ±‡æ€»ä¿¡æ¯å­˜å‚¨åœ¨ `data_cleaning_batches.summary_report`
2. **æ–‡ä»¶å­˜å‚¨**ï¼ˆå¯é€‰ï¼‰: å®Œæ•´çš„ Markdown æŠ¥å‘Šå­˜å‚¨ä¸ºæ–‡ä»¶

```python
async def save_report_to_file(batch_id: str, report: str):
    """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    
    report_dir = Path('reports/cleaning')
    report_dir.mkdir(parents=True, exist_ok=True)
    
    filename = f"{batch_id}.md"
    filepath = report_dir / filename
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    return str(filepath)
```

---

## ç‰ˆæœ¬å†å²

- **v1.0** (2026-01-27): åˆå§‹æŠ¥å‘Šç³»ç»Ÿè®¾è®¡
  - æ‰¹æ¬¡æŠ¥å‘Š
  - å‘¨æœŸæŠ¥å‘Š
  - è´¨é‡æŒ‡æ ‡
  - è¶‹åŠ¿åˆ†æ
